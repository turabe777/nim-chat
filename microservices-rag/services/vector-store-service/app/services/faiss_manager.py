"""
FAISS ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹

FAISSã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æ©Ÿèƒ½
"""

import faiss
import numpy as np
import json
import aiofiles
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
from uuid import UUID
from datetime import datetime
import pickle

from shared.models.vector_store import VectorStoreConfig, VectorIndexStats
from shared.utils.logging import get_logger
from shared.utils.exceptions import ProcessingError, DocumentNotFoundError

logger = get_logger(__name__)


class FAISSManagerService:
    """FAISS ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self, storage_path: str):
        self.storage_path = Path(storage_path)
        self.indexes_dir = self.storage_path / "faiss_indexes"
        self.metadata_dir = self.storage_path / "index_metadata"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self._ensure_directories()
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._index_cache: Dict[str, faiss.Index] = {}
        self._metadata_cache: Dict[str, Dict] = {}
    
    def _ensure_directories(self) -> None:
        """å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
        self.indexes_dir.mkdir(parents=True, exist_ok=True)
        self.metadata_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ FAISS storage directories ensured: {self.storage_path}")
    
    def _get_index_file_path(self, document_id: UUID) -> Path:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        return self.indexes_dir / f"{document_id}.faiss"
    
    def _get_metadata_file_path(self, document_id: UUID) -> Path:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        return self.metadata_dir / f"{document_id}_index.json"
    
    def _get_mapping_file_path(self, document_id: UUID) -> Path:
        """ID ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        return self.metadata_dir / f"{document_id}_mapping.pkl"
    
    def _create_faiss_index(
        self, 
        vectors: np.ndarray, 
        config: VectorStoreConfig
    ) -> faiss.Index:
        """FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ"""
        
        dimension = vectors.shape[1]
        n_vectors = vectors.shape[0]
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        if config.index_type == "Flat":
            if config.metric_type == "IP":
                index = faiss.IndexFlatIP(dimension)
            else:  # L2
                index = faiss.IndexFlatL2(dimension)
                
        elif config.index_type == "IVFFlat":
            # ã‚¯ãƒ©ã‚¹ã‚¿æ•°èª¿æ•´ï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ•°ãŒå°‘ãªã„å ´åˆï¼‰
            nlist = min(config.nlist, max(1, n_vectors // 4))
            
            if config.metric_type == "IP":
                quantizer = faiss.IndexFlatIP(dimension)
                index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
            else:  # L2
                quantizer = faiss.IndexFlatL2(dimension)
                index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
            
            # å­¦ç¿’ãŒå¿…è¦
            if n_vectors >= nlist:
                index.train(vectors)
            else:
                # ãƒ™ã‚¯ãƒˆãƒ«æ•°ãŒå°‘ãªã„å ´åˆã¯Flatã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                logger.warning(f"âš ï¸ Too few vectors ({n_vectors}) for IVFFlat, using Flat index")
                if config.metric_type == "IP":
                    index = faiss.IndexFlatIP(dimension)
                else:
                    index = faiss.IndexFlatL2(dimension)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Flat
            if config.metric_type == "IP":
                index = faiss.IndexFlatIP(dimension)
            else:
                index = faiss.IndexFlatL2(dimension)
        
        return index
    
    async def create_index(
        self,
        document_id: UUID,
        vectors: List[List[float]],
        embedding_ids: List[UUID],
        chunk_ids: List[UUID],
        config: Optional[VectorStoreConfig] = None
    ) -> Dict[str, Any]:
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ"""
        
        if not vectors or len(vectors) != len(embedding_ids) != len(chunk_ids):
            raise ProcessingError("index_creation", "Vector, embedding_id, and chunk_id lists must have same length")
        
        if not config:
            config = VectorStoreConfig()
        
        start_time = datetime.now()
        
        try:
            # ãƒ™ã‚¯ãƒˆãƒ«å…¥åŠ›æ¤œè¨¼
            if not vectors or not isinstance(vectors, list):
                raise ProcessingError("index_creation", "Invalid vector input: must be a non-empty list")
            
            # ãƒ™ã‚¯ãƒˆãƒ«ã‚’numpyé…åˆ—ã«å¤‰æ›
            vectors_array = np.array(vectors, dtype=np.float32)
            
            # ãƒ‡ãƒãƒƒã‚°: ãƒ™ã‚¯ãƒˆãƒ«é…åˆ—ã®å½¢çŠ¶ã‚’ç¢ºèª
            logger.info(f"ğŸ” Vector array shape: {vectors_array.shape}")
            logger.info(f"ğŸ” Expected dimension: 384 (all-MiniLM-L6-v2)")
            
            # æ¬¡å…ƒæ•°æ¤œè¨¼
            if len(vectors_array.shape) != 2:
                raise ProcessingError("index_creation", f"Invalid vector shape: expected 2D array, got {vectors_array.shape}")
            
            expected_dim = 384  # all-MiniLM-L6-v2ã®æ¨™æº–æ¬¡å…ƒæ•°
            actual_dim = vectors_array.shape[1]
            
            if actual_dim != expected_dim:
                logger.error(f"âŒ Dimension mismatch: expected {expected_dim}, got {actual_dim}")
                logger.error(f"âŒ Vector input sample: {vectors[0][:5] if len(vectors) > 0 and hasattr(vectors[0], '__len__') else 'No sample'}")
                # å¼·åˆ¶çš„ã«384æ¬¡å…ƒã¨ã—ã¦å‡¦ç†ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¾ãŸã¯åˆ‡ã‚Šè©°ã‚ï¼‰
                if actual_dim < expected_dim:
                    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆã‚¼ãƒ­åŸ‹ã‚ï¼‰
                    padding = np.zeros((vectors_array.shape[0], expected_dim - actual_dim), dtype=np.float32)
                    vectors_array = np.hstack([vectors_array, padding])
                    logger.warning(f"âš ï¸ Padded vectors from {actual_dim} to {expected_dim} dimensions")
                else:
                    # åˆ‡ã‚Šè©°ã‚
                    vectors_array = vectors_array[:, :expected_dim]
                    logger.warning(f"âš ï¸ Truncated vectors from {actual_dim} to {expected_dim} dimensions")
            
            # ãƒ™ã‚¯ãƒˆãƒ«æ­£è¦åŒ–
            if config.normalize_vectors:
                faiss.normalize_L2(vectors_array)
                logger.debug(f"ğŸ“ Normalized {len(vectors)} vectors")
            
            # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            index = self._create_faiss_index(vectors_array, config)
            
            # ãƒ™ã‚¯ãƒˆãƒ«è¿½åŠ 
            index.add(vectors_array)
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¿å­˜
            index_file = self._get_index_file_path(document_id)
            faiss.write_index(index, str(index_file))
            
            # ID ãƒãƒƒãƒ”ãƒ³ã‚°ä¿å­˜
            mapping_data = {
                "embedding_ids": [str(eid) for eid in embedding_ids],
                "chunk_ids": [str(cid) for cid in chunk_ids]
            }
            mapping_file = self._get_mapping_file_path(document_id)
            async with aiofiles.open(mapping_file, 'wb') as f:
                await f.write(pickle.dumps(mapping_data))
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            processing_time = (datetime.now() - start_time).total_seconds()
            index_size_mb = index_file.stat().st_size / (1024 * 1024)
            
            metadata = {
                "document_id": str(document_id),
                "total_vectors": len(vectors),
                "dimension": vectors_array.shape[1],
                "index_type": config.index_type,
                "metric_type": config.metric_type,
                "index_size_mb": round(index_size_mb, 3),
                "creation_time": processing_time,
                "created_at": datetime.utcnow().isoformat(),
                "config": config.model_dump()
            }
            
            metadata_file = self._get_metadata_file_path(document_id)
            async with aiofiles.open(metadata_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(metadata, ensure_ascii=False, indent=2, default=str))
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
            cache_key = str(document_id)
            self._index_cache[cache_key] = index
            self._metadata_cache[cache_key] = metadata
            
            logger.info(f"ğŸ” Created FAISS index for document {document_id} ({len(vectors)} vectors, {processing_time:.3f}s)")
            
            return {
                "document_id": document_id,
                "indexed_count": len(vectors),
                "index_size_mb": index_size_mb,
                "creation_time": processing_time,
                "index_info": {
                    "type": config.index_type,
                    "metric": config.metric_type,
                    "dimension": vectors_array.shape[1]
                }
            }
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            for file_path in [
                self._get_index_file_path(document_id),
                self._get_metadata_file_path(document_id),
                self._get_mapping_file_path(document_id)
            ]:
                if file_path.exists():
                    file_path.unlink()
            
            raise ProcessingError("index_creation", f"Failed to create FAISS index: {str(e)}")
    
    async def _load_index(self, document_id: UUID) -> Tuple[faiss.Index, Dict]:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿"""
        
        cache_key = str(document_id)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if cache_key in self._index_cache:
            mapping_file = self._get_mapping_file_path(document_id)
            async with aiofiles.open(mapping_file, 'rb') as f:
                mapping_data = pickle.loads(await f.read())
            return self._index_cache[cache_key], mapping_data
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        index_file = self._get_index_file_path(document_id)
        mapping_file = self._get_mapping_file_path(document_id)
        
        if not index_file.exists() or not mapping_file.exists():
            raise DocumentNotFoundError(f"Index not found for document {document_id}")
        
        try:
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿
            index = faiss.read_index(str(index_file))
            
            # ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿
            async with aiofiles.open(mapping_file, 'rb') as f:
                mapping_data = pickle.loads(await f.read())
                
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            self._index_cache[cache_key] = index
            
            return index, mapping_data
            
        except Exception as e:
            raise ProcessingError("index_loading", f"Failed to load index: {str(e)}")
    
    async def search_vectors(
        self,
        document_id: UUID,
        query_vector: List[float],
        top_k: int = 5,
        similarity_threshold: float = 0.0
    ) -> List[Dict[str, Any]]:
        """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        
        try:
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿
            index, mapping_data = await self._load_index(document_id)
            
            # ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«æº–å‚™
            query_array = np.array([query_vector], dtype=np.float32)
            
            # æ­£è¦åŒ–ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆæ™‚ã¨åŒã˜å‡¦ç†ï¼‰
            faiss.normalize_L2(query_array)
            
            # æ¤œç´¢å®Ÿè¡Œ
            scores, indices = index.search(query_array, min(top_k, index.ntotal))
            
            # çµæœå‡¦ç†
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx == -1:  # ç„¡åŠ¹ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                    continue
                    
                if score < similarity_threshold:
                    continue
                
                embedding_id = UUID(mapping_data["embedding_ids"][idx])
                chunk_id = UUID(mapping_data["chunk_ids"][idx])
                
                results.append({
                    "embedding_id": embedding_id,
                    "chunk_id": chunk_id,
                    "document_id": document_id,
                    "similarity_score": float(score),
                    "rank": i + 1
                })
            
            logger.debug(f"ğŸ” Vector search found {len(results)} results for document {document_id}")
            return results
            
        except Exception as e:
            raise ProcessingError("vector_search", f"Failed to search vectors: {str(e)}")
    
    async def get_index_stats(self, document_id: UUID) -> Optional[VectorIndexStats]:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        
        metadata_file = self._get_metadata_file_path(document_id)
        
        if not metadata_file.exists():
            return None
        
        try:
            async with aiofiles.open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.loads(await f.read())
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¨ˆç®—ï¼ˆæ¦‚ç®—ï¼‰
            memory_usage_mb = metadata.get("index_size_mb", 0) * 1.2  # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®1.2å€ç¨‹åº¦
            
            return VectorIndexStats(
                total_vectors=metadata["total_vectors"],
                dimension=metadata["dimension"],
                index_type=metadata["index_type"],
                memory_usage_mb=round(memory_usage_mb, 3),
                last_updated=datetime.fromisoformat(metadata["created_at"]),
                documents_count=1
            )
            
        except Exception as e:
            logger.error(f"âŒ Failed to get index stats for {document_id}: {str(e)}")
            return None
    
    async def delete_index(self, document_id: UUID) -> bool:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤"""
        
        deleted = False
        cache_key = str(document_id)
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            for file_path in [
                self._get_index_file_path(document_id),
                self._get_metadata_file_path(document_id),
                self._get_mapping_file_path(document_id)
            ]:
                if file_path.exists():
                    file_path.unlink()
                    deleted = True
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
            if cache_key in self._index_cache:
                del self._index_cache[cache_key]
            if cache_key in self._metadata_cache:
                del self._metadata_cache[cache_key]
            
            if deleted:
                logger.info(f"ğŸ—‘ï¸ Deleted FAISS index for document: {document_id}")
            
            return deleted
            
        except Exception as e:
            raise ProcessingError("index_deletion", f"Failed to delete index: {str(e)}")
    
    async def list_indexes(self) -> List[Dict[str, Any]]:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§ã‚’å–å¾—"""
        
        indexes = []
        
        try:
            for metadata_file in self.metadata_dir.glob("*_index.json"):
                try:
                    async with aiofiles.open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.loads(await f.read())
                    indexes.append(metadata)
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to load index metadata {metadata_file}: {str(e)}")
                    continue
            
            # ä½œæˆæ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
            indexes.sort(
                key=lambda idx: idx.get('created_at', ''),
                reverse=True
            )
            
            return indexes
            
        except Exception as e:
            raise ProcessingError("index_listing", f"Failed to list indexes: {str(e)}")