"""
ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã‚µãƒ¼ãƒ“ã‚¹

ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®šå¯èƒ½ãªã‚µã‚¤ã‚ºãƒ»ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã§ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
"""

import re
from typing import List, Dict, Any, Optional
from uuid import UUID
import time

from shared.models.text_processing import TextChunk, TextProcessingConfig
from shared.utils.logging import get_logger
from shared.utils.exceptions import ProcessingError

logger = get_logger(__name__)


class TextSplitterService:
    """ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        self.default_separators = ["\n\n", "\n", "ã€‚", ".", " ", ""]
    
    async def split_text(
        self, 
        document_id: UUID, 
        text: str, 
        config: TextProcessingConfig
    ) -> List[TextChunk]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        
        start_time = time.time()
        logger.info(f"ğŸ“„ Splitting text for document {document_id} (length: {len(text)})")
        
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
            processed_text = self._preprocess_text(text, config)
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunks = self._split_into_chunks(
                document_id=document_id,
                text=processed_text,
                chunk_size=config.chunk_size,
                chunk_overlap=config.chunk_overlap,
                separators=config.separators or self.default_separators
            )
            
            # æœ€å°ã‚µã‚¤ã‚ºãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_chunks = [
                chunk for chunk in chunks 
                if len(chunk.content) >= config.min_chunk_size
            ]
            
            processing_time = time.time() - start_time
            logger.info(f"âœ… Split into {len(filtered_chunks)} chunks in {processing_time:.2f}s")
            
            return filtered_chunks
            
        except Exception as e:
            raise ProcessingError("text_splitting", f"Failed to split text: {str(e)}")
    
    def _preprocess_text(self, text: str, config: TextProcessingConfig) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
        
        if config.remove_whitespace:
            # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
            text = re.sub(r'\s+', ' ', text)
        
        if config.remove_empty_lines:
            # ç©ºè¡Œã‚’å‰Šé™¤
            lines = text.split('\n')
            text = '\n'.join(line for line in lines if line.strip())
        
        return text.strip()
    
    def _split_into_chunks(
        self,
        document_id: UUID,
        text: str,
        chunk_size: int,
        chunk_overlap: int,
        separators: List[str]
    ) -> List[TextChunk]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’å†å¸°çš„ã«ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        
        chunks = []
        current_pos = 0
        chunk_index = 0
        
        while current_pos < len(text):
            # ãƒãƒ£ãƒ³ã‚¯ã®çµ‚äº†ä½ç½®ã‚’è¨ˆç®—
            end_pos = min(current_pos + chunk_size, len(text))
            
            # é©åˆ‡ãªåˆ†å‰²ç‚¹ã‚’æ¢ã™
            best_split_pos = self._find_best_split_position(
                text, current_pos, end_pos, separators
            )
            
            # ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
            chunk_content = text[current_pos:best_split_pos].strip()
            
            if chunk_content:  # ç©ºã§ãªã„ãƒãƒ£ãƒ³ã‚¯ã®ã¿è¿½åŠ 
                chunk = TextChunk(
                    document_id=document_id,
                    content=chunk_content,
                    chunk_index=chunk_index,
                    start_position=current_pos,
                    end_position=best_split_pos,
                    metadata={
                        "length": len(chunk_content),
                        "separator_used": self._get_separator_used(text, best_split_pos, separators)
                    }
                )
                chunks.append(chunk)
                chunk_index += 1
            
            # æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯ã®é–‹å§‹ä½ç½®ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è€ƒæ…®ï¼‰
            next_start = max(current_pos + 1, best_split_pos - chunk_overlap)
            
            # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
            if next_start <= current_pos:
                next_start = current_pos + 1
            
            current_pos = next_start
        
        return chunks
    
    def _find_best_split_position(
        self, 
        text: str, 
        start_pos: int, 
        max_end_pos: int, 
        separators: List[str]
    ) -> int:
        """æœ€é©ãªåˆ†å‰²ä½ç½®ã‚’æ¢ã™"""
        
        if max_end_pos >= len(text):
            return len(text)
        
        # å„ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ãƒ¼ã§æœ€é©ãªåˆ†å‰²ç‚¹ã‚’æ¢ã™
        for separator in separators:
            if not separator:  # ç©ºæ–‡å­—ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆæ–‡å­—å˜ä½åˆ†å‰²ï¼‰
                return max_end_pos
            
            # ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ãƒ¼ä½ç½®ã‚’å¾Œã‚ã‹ã‚‰æ¢ã™
            search_start = max(start_pos, max_end_pos - len(separator))
            pos = text.rfind(separator, search_start, max_end_pos)
            
            if pos != -1 and pos > start_pos:
                return pos + len(separator)
        
        # ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€å¤§ä½ç½®ã§åˆ†å‰²
        return max_end_pos
    
    def _get_separator_used(self, text: str, pos: int, separators: List[str]) -> str:
        """ä½¿ç”¨ã•ã‚ŒãŸã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ç‰¹å®š"""
        
        for separator in separators:
            if not separator:
                continue
            if pos >= len(separator) and text[pos-len(separator):pos] == separator:
                return separator
        
        return "none"
    
    async def get_text_statistics(self, chunks: List[TextChunk]) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆçµ±è¨ˆã‚’è¨ˆç®—"""
        
        if not chunks:
            return {
                "total_chunks": 0,
                "total_characters": 0,
                "average_chunk_size": 0.0,
                "min_chunk_size": 0,
                "max_chunk_size": 0
            }
        
        chunk_sizes = [len(chunk.content) for chunk in chunks]
        
        return {
            "total_chunks": len(chunks),
            "total_characters": sum(chunk_sizes),
            "average_chunk_size": sum(chunk_sizes) / len(chunk_sizes),
            "min_chunk_size": min(chunk_sizes),
            "max_chunk_size": max(chunk_sizes)
        }