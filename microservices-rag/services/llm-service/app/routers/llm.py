"""
LLM API

è³ªå•å¿œç­”ãƒ»LLMæ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
"""

from fastapi import APIRouter, HTTPException, Depends
from typing import Optional
import time

from shared.models.llm import (
    QuestionAnsweringRequest, QuestionAnsweringResponse,
    LLMGenerationRequest, LLMGenerationResponse,
    ModelListResponse, ConnectionTestResponse
)
from shared.utils.config import LLMServiceConfig
from shared.utils.logging import get_logger
from shared.utils.exceptions import ProcessingError
from ..services.nvidia_client import NVIDIACloudClient
from ..services.rag_pipeline import RAGPipeline

router = APIRouter()
logger = get_logger(__name__)

# ä¾å­˜é–¢ä¿‚æ³¨å…¥
def get_config() -> LLMServiceConfig:
    return LLMServiceConfig()

def get_nvidia_client(config: LLMServiceConfig = Depends(get_config)) -> NVIDIACloudClient:
    return NVIDIACloudClient(config)

def get_rag_pipeline(config: LLMServiceConfig = Depends(get_config)) -> RAGPipeline:
    return RAGPipeline(config)


@router.post("/qa", response_model=QuestionAnsweringResponse)
async def question_answering(
    request: QuestionAnsweringRequest,
    rag_pipeline: RAGPipeline = Depends(get_rag_pipeline)
):
    """è³ªå•å¿œç­”API - RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    logger.info(f"ğŸ¯ Question answering request: {request.question[:50]}...")
    
    try:
        result = await rag_pipeline.answer_question(
            question=request.question,
            document_id=request.document_id,
            context_length=request.context_length,
            similarity_threshold=request.similarity_threshold,
            model=request.model,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        
        response = QuestionAnsweringResponse(
            service="llm-service",
            question=request.question,
            answer=result["answer"],
            confidence=result["confidence"],
            contexts_used=result["contexts_used"],
            total_contexts_found=result["total_contexts_found"],
            model_used=result["model_used"],
            processing_time=result["processing_time"],
            token_usage=result["token_usage"],
            request_id=result["request_id"]
        )
        
        logger.info(f"âœ… Question answered successfully in {result['processing_time']:.3f}s")
        return response
        
    except ProcessingError as e:
        logger.error(f"RAG processing error: {e.message}")
        raise HTTPException(status_code=400, detail=f"Processing error: {e.message}")
    except Exception as e:
        logger.error(f"Unexpected error in question answering: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")


@router.post("/generate", response_model=LLMGenerationResponse)
async def generate_text(
    request: LLMGenerationRequest,
    nvidia_client: NVIDIACloudClient = Depends(get_nvidia_client)
):
    """ãƒ†ã‚­ã‚¹ãƒˆç”ŸæˆAPI - ç›´æ¥LLMå‘¼ã³å‡ºã—"""
    
    logger.info(f"âœï¸ Text generation request: {request.prompt[:50]}...")
    
    try:
        result = await nvidia_client.generate_response(
            question=request.prompt,
            context="",  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—ã®ç›´æ¥ç”Ÿæˆ
            model=request.model,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        
        response = LLMGenerationResponse(
            service="llm-service",
            prompt=request.prompt,
            generated_text=result["answer"],
            model_used=result["model_used"],
            processing_time=result["processing_time"],
            token_usage=result["token_usage"],
            request_id=result["request_id"]
        )
        
        logger.info(f"âœ… Text generated successfully in {result['processing_time']:.3f}s")
        return response
        
    except ProcessingError as e:
        logger.error(f"LLM generation error: {e.message}")
        raise HTTPException(status_code=400, detail=f"Generation error: {e.message}")
    except Exception as e:
        logger.error(f"Unexpected error in text generation: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")


@router.get("/models", response_model=ModelListResponse)
async def list_models(config: LLMServiceConfig = Depends(get_config)):
    """åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ä¸€è¦§"""
    
    logger.info("ğŸ“‹ Model list request")
    
    # NVIDIA NIM ã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§
    available_models = [
        {
            "name": "nvidia/llama-3.1-nemotron-70b-instruct",
            "description": "é«˜æ€§èƒ½ãª70Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«",
            "max_tokens": 2048,
            "context_length": 128000,
            "recommended_use": "æ±ç”¨è³ªå•å¿œç­”"
        },
        {
            "name": "nvidia/llama-3.1-nemotron-51b-instruct",
            "description": "ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸ51Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«",
            "max_tokens": 2048,
            "context_length": 128000,
            "recommended_use": "åŠ¹ç‡çš„ãªè³ªå•å¿œç­”"
        },
        {
            "name": "meta/llama-3.1-8b-instruct",
            "description": "è»½é‡ãª8Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«",
            "max_tokens": 2048,
            "context_length": 128000,
            "recommended_use": "é«˜é€Ÿå¿œç­”"
        }
    ]
    
    return ModelListResponse(
        service="llm-service",
        available_models=available_models,
        default_model="nvidia/llama-3.1-nemotron-70b-instruct"
    )


@router.get("/test", response_model=ConnectionTestResponse)
async def test_connection(nvidia_client: NVIDIACloudClient = Depends(get_nvidia_client)):
    """NVIDIA APIæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
    
    logger.info("ğŸ”Œ Testing NVIDIA API connection")
    
    start_time = time.time()
    
    try:
        result = await nvidia_client.test_connection()
        response_time = time.time() - start_time
        
        return ConnectionTestResponse(
            service="llm-service",
            connection_status=result["status"],
            message=result["message"],
            model_tested=result.get("model"),
            response_time=response_time
        )
        
    except Exception as e:
        logger.error(f"Connection test failed: {e}")
        return ConnectionTestResponse(
            service="llm-service",
            connection_status="error",
            message=f"Connection test failed: {str(e)}",
            response_time=time.time() - start_time
        )