"""
ãƒãƒ£ãƒ³ã‚¯ç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹

åˆ†å‰²ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®ä¿å­˜ãƒ»å–å¾—ãƒ»ç®¡ç†
"""

import json
import aiofiles
from pathlib import Path
from typing import List, Optional, Dict, Any
from uuid import UUID
from datetime import datetime

from shared.models.text_processing import TextChunk
from shared.utils.logging import get_logger
from shared.utils.exceptions import ProcessingError, DocumentNotFoundError

logger = get_logger(__name__)


class ChunkManagerService:
    """ãƒãƒ£ãƒ³ã‚¯ç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self, storage_path: str):
        self.storage_path = Path(storage_path)
        self.chunks_dir = self.storage_path / "chunks"
        self.metadata_dir = self.storage_path / "chunk_metadata"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self._ensure_directories()
    
    def _ensure_directories(self) -> None:
        """å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
        self.chunks_dir.mkdir(parents=True, exist_ok=True)
        self.metadata_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ Chunk storage directories ensured: {self.storage_path}")
    
    def _get_chunks_file_path(self, document_id: UUID) -> Path:
        """ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        return self.chunks_dir / f"{document_id}.json"
    
    def _get_metadata_file_path(self, document_id: UUID) -> Path:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        return self.metadata_dir / f"{document_id}_metadata.json"
    
    async def save_chunks(self, document_id: UUID, chunks: List[TextChunk]) -> bool:
        """ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜"""
        
        chunks_file = self._get_chunks_file_path(document_id)
        metadata_file = self._get_metadata_file_path(document_id)
        
        try:
            # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            chunks_data = [chunk.model_dump() for chunk in chunks]
            async with aiofiles.open(chunks_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(chunks_data, ensure_ascii=False, indent=2, default=str))
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            metadata = {
                "document_id": str(document_id),
                "total_chunks": len(chunks),
                "created_at": datetime.utcnow().isoformat(),
                "total_characters": sum(len(chunk.content) for chunk in chunks),
                "chunk_sizes": [len(chunk.content) for chunk in chunks]
            }
            
            async with aiofiles.open(metadata_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(metadata, ensure_ascii=False, indent=2, default=str))
            
            logger.info(f"ğŸ’¾ Saved {len(chunks)} chunks for document {document_id}")
            return True
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if chunks_file.exists():
                chunks_file.unlink()
            if metadata_file.exists():
                metadata_file.unlink()
            
            raise ProcessingError("chunk_save", f"Failed to save chunks: {str(e)}")
    
    async def get_chunks(
        self, 
        document_id: UUID, 
        limit: Optional[int] = None, 
        offset: int = 0
    ) -> List[TextChunk]:
        """ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—"""
        
        chunks_file = self._get_chunks_file_path(document_id)
        
        if not chunks_file.exists():
            raise DocumentNotFoundError(f"Chunks not found for document {document_id}")
        
        try:
            async with aiofiles.open(chunks_file, 'r', encoding='utf-8') as f:
                chunks_data = json.loads(await f.read())
            
            # Pydanticãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›
            chunks = [TextChunk.model_validate(chunk_data) for chunk_data in chunks_data]
            
            # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨
            if limit:
                chunks = chunks[offset:offset + limit]
            elif offset > 0:
                chunks = chunks[offset:]
            
            logger.debug(f"ğŸ“– Retrieved {len(chunks)} chunks for document {document_id}")
            return chunks
            
        except Exception as e:
            raise ProcessingError("chunk_retrieval", f"Failed to retrieve chunks: {str(e)}")
    
    async def get_chunk_by_id(self, document_id: UUID, chunk_id: UUID) -> Optional[TextChunk]:
        """æŒ‡å®šã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—"""
        
        chunks = await self.get_chunks(document_id)
        
        for chunk in chunks:
            if chunk.chunk_id == chunk_id:
                return chunk
        
        return None
    
    async def get_chunk_metadata(self, document_id: UUID) -> Optional[Dict[str, Any]]:
        """ãƒãƒ£ãƒ³ã‚¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        
        metadata_file = self._get_metadata_file_path(document_id)
        
        if not metadata_file.exists():
            return None
        
        try:
            async with aiofiles.open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.loads(await f.read())
            return metadata
            
        except Exception as e:
            logger.error(f"âŒ Failed to load chunk metadata {document_id}: {str(e)}")
            return None
    
    async def delete_chunks(self, document_id: UUID) -> bool:
        """ãƒãƒ£ãƒ³ã‚¯ã‚’å‰Šé™¤"""
        
        chunks_file = self._get_chunks_file_path(document_id)
        metadata_file = self._get_metadata_file_path(document_id)
        
        deleted = False
        
        try:
            if chunks_file.exists():
                chunks_file.unlink()
                deleted = True
            
            if metadata_file.exists():
                metadata_file.unlink()
                deleted = True
            
            if deleted:
                logger.info(f"ğŸ—‘ï¸ Deleted chunks for document: {document_id}")
            
            return deleted
            
        except Exception as e:
            raise ProcessingError("chunk_deletion", f"Failed to delete chunks: {str(e)}")
    
    async def list_processed_documents(self) -> List[Dict[str, Any]]:
        """å‡¦ç†æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§ã‚’å–å¾—"""
        
        documents = []
        
        try:
            for metadata_file in self.metadata_dir.glob("*_metadata.json"):
                try:
                    async with aiofiles.open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata = json.loads(await f.read())
                    documents.append(metadata)
                except Exception as e:
                    logger.warning(f"âš ï¸ Failed to load metadata file {metadata_file}: {str(e)}")
                    continue
            
            # ä½œæˆæ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
            documents.sort(
                key=lambda doc: doc.get('created_at', ''),
                reverse=True
            )
            
            return documents
            
        except Exception as e:
            raise ProcessingError("document_listing", f"Failed to list processed documents: {str(e)}")
    
    async def get_statistics(self, document_id: UUID) -> Optional[Dict[str, Any]]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        
        metadata = await self.get_chunk_metadata(document_id)
        if not metadata:
            return None
        
        chunk_sizes = metadata.get('chunk_sizes', [])
        if not chunk_sizes:
            return metadata
        
        # çµ±è¨ˆè¨ˆç®—
        stats = {
            **metadata,
            "average_chunk_size": sum(chunk_sizes) / len(chunk_sizes),
            "min_chunk_size": min(chunk_sizes),
            "max_chunk_size": max(chunk_sizes)
        }
        
        return stats