"""
Vector Store Service API Routes

ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ»é¡ä¼¼åº¦æ¤œç´¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
"""

from fastapi import APIRouter, Depends, HTTPException
from typing import List, Optional
from uuid import UUID
from datetime import datetime
import time

from shared.models.vector_store import (
    VectorIndexRequest, VectorIndexResponse,
    VectorSearchQuery, VectorSearchResponse, VectorSearchResult,
    SimilaritySearchRequest, SimilaritySearchResponse,
    VectorStoreConfig, VectorIndexStats
)
from shared.utils.config import VectorStoreServiceConfig
from shared.utils.logging import get_logger
from shared.utils.exceptions import ProcessingError, DocumentNotFoundError
from ..services.faiss_manager import FAISSManagerService

router = APIRouter()
config = VectorStoreServiceConfig()
logger = get_logger(__name__)

# ã‚µãƒ¼ãƒ“ã‚¹ä¾å­˜é–¢æ•°
def get_faiss_manager():
    """FAISSç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹ã‚’å–å¾—"""
    return FAISSManagerService(config.storage_path)


@router.post("/vector/index", response_model=VectorIndexResponse)
async def create_vector_index(
    request: VectorIndexRequest,
    faiss_manager: FAISSManagerService = Depends(get_faiss_manager)
):
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ"""
    
    logger.info(f"ğŸ” Creating vector index for document {request.document_id}")
    
    try:
        # è¨­å®šæº–å‚™
        vector_config = VectorStoreConfig()
        if request.index_config:
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®è¨­å®šã§ä¸Šæ›¸ã
            for key, value in request.index_config.items():
                if hasattr(vector_config, key):
                    setattr(vector_config, key, value)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        result = await faiss_manager.create_index(
            document_id=request.document_id,
            vectors=request.embedding_vectors,
            embedding_ids=request.embedding_ids,
            chunk_ids=request.chunk_ids,
            config=vector_config
        )
        
        logger.info(f"âœ… Created vector index for document {request.document_id}")
        
        return VectorIndexResponse(**result)
        
    except Exception as e:
        logger.error(f"âŒ Failed to create vector index: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/vector/search", response_model=VectorSearchResponse)
async def search_vectors(
    query: VectorSearchQuery,
    faiss_manager: FAISSManagerService = Depends(get_faiss_manager)
):
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ"""
    
    start_time = time.time()
    
    logger.info(f"ğŸ” Vector search for document {query.document_id}")
    
    try:
        if not query.document_id:
            raise ProcessingError("search_error", "document_id is required for vector search")
        
        # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ
        search_results = await faiss_manager.search_vectors(
            document_id=query.document_id,
            query_vector=query.query_vector,
            top_k=query.top_k,
            similarity_threshold=query.similarity_threshold
        )
        
        # çµæœå¤‰æ›
        results = []
        for result in search_results:
            vector_result = VectorSearchResult(
                embedding_id=result["embedding_id"],
                chunk_id=result["chunk_id"],
                document_id=result["document_id"],
                similarity_score=result["similarity_score"],
                metadata={"rank": result["rank"]} if query.include_metadata else None
            )
            results.append(vector_result)
        
        search_time = time.time() - start_time
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±å–å¾—
        index_stats = await faiss_manager.get_index_stats(query.document_id)
        index_info = {
            "total_vectors": index_stats.total_vectors if index_stats else 0,
            "dimension": index_stats.dimension if index_stats else 0,
            "index_type": index_stats.index_type if index_stats else "unknown"
        }
        
        logger.info(f"âœ… Vector search completed: {len(results)} results in {search_time:.3f}s")
        
        return VectorSearchResponse(
            results=results,
            total_found=len(results),
            search_time=search_time,
            index_info=index_info
        )
        
    except DocumentNotFoundError as e:
        logger.error(f"âŒ Document not found: {str(e)}")
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"âŒ Vector search failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/vector/similarity", response_model=SimilaritySearchResponse)
async def similarity_search(
    request: SimilaritySearchRequest,
    faiss_manager: FAISSManagerService = Depends(get_faiss_manager)
):
    """é¡ä¼¼åº¦æ¤œç´¢ã‚’å®Ÿè¡Œ"""
    
    start_time = time.time()
    
    logger.info(f"ğŸ” Similarity search request")
    
    try:
        if not request.query_vector:
            raise ProcessingError("search_error", "query_vector is required for similarity search")
        
        # å¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢
        if not request.document_ids:
            # å…¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§å–å¾—
            all_indexes = await faiss_manager.list_indexes()
            document_ids = [UUID(idx["document_id"]) for idx in all_indexes]
        else:
            document_ids = request.document_ids
        
        # å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§æ¤œç´¢å®Ÿè¡Œ
        all_results = []
        for doc_id in document_ids:
            try:
                doc_results = await faiss_manager.search_vectors(
                    document_id=doc_id,
                    query_vector=request.query_vector,
                    top_k=request.top_k * 2,  # å¤šã‚ã«å–å¾—ã—ã¦ã‹ã‚‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    similarity_threshold=request.similarity_threshold
                )
                all_results.extend(doc_results)
            except DocumentNotFoundError:
                logger.warning(f"âš ï¸ Index not found for document {doc_id}")
                continue
        
        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½Kä»¶å–å¾—
        all_results.sort(key=lambda x: x["similarity_score"], reverse=True)
        top_results = all_results[:request.top_k]
        
        # çµæœå¤‰æ›
        results = []
        for result in top_results:
            vector_result = VectorSearchResult(
                embedding_id=result["embedding_id"],
                chunk_id=result["chunk_id"],
                document_id=result["document_id"],
                similarity_score=result["similarity_score"],
                vector=request.query_vector if request.include_vectors else None,
                metadata={"rank": result["rank"]} if request.include_content else None
            )
            results.append(vector_result)
        
        search_time = time.time() - start_time
        
        logger.info(f"âœ… Similarity search completed: {len(results)} results in {search_time:.3f}s")
        
        return SimilaritySearchResponse(
            query_info={
                "query_type": "vector",
                "target_documents": len(document_ids),
                "similarity_threshold": request.similarity_threshold
            },
            results=results,
            total_found=len(results),
            search_stats={
                "search_time": search_time,
                "total_candidates": len(all_results),
                "documents_searched": len(document_ids)
            }
        )
        
    except Exception as e:
        logger.error(f"âŒ Similarity search failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/vector/stats/{document_id}", response_model=VectorIndexStats)
async def get_index_stats(
    document_id: UUID,
    faiss_manager: FAISSManagerService = Depends(get_faiss_manager)
):
    """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    
    logger.info(f"ğŸ“Š Getting index stats for document {document_id}")
    
    try:
        stats = await faiss_manager.get_index_stats(document_id)
        
        if not stats:
            raise HTTPException(status_code=404, detail=f"Index not found for document {document_id}")
        
        logger.info(f"âœ… Retrieved index stats for document {document_id}")
        return stats
        
    except Exception as e:
        logger.error(f"âŒ Failed to get index stats: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/vector/indexes")
async def list_indexes(
    faiss_manager: FAISSManagerService = Depends(get_faiss_manager)
):
    """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸€è¦§ã‚’å–å¾—"""
    
    logger.info("ğŸ“‹ Listing vector indexes")
    
    try:
        indexes = await faiss_manager.list_indexes()
        
        logger.info(f"ğŸ“Š Found {len(indexes)} vector indexes")
        
        return {
            "indexes": indexes,
            "total_count": len(indexes)
        }
        
    except Exception as e:
        logger.error(f"âŒ Failed to list indexes: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/vector/index/{document_id}")
async def delete_index(
    document_id: UUID,
    faiss_manager: FAISSManagerService = Depends(get_faiss_manager)
):
    """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‰Šé™¤"""
    
    logger.info(f"ğŸ—‘ï¸ Deleting vector index for document {document_id}")
    
    try:
        deleted = await faiss_manager.delete_index(document_id)
        
        if not deleted:
            raise HTTPException(status_code=404, detail=f"Index not found for document {document_id}")
        
        logger.info(f"âœ… Deleted vector index for document {document_id}")
        
        return {
            "message": "Index deleted successfully",
            "document_id": document_id
        }
        
    except Exception as e:
        logger.error(f"âŒ Failed to delete index: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))